<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xin Jin</title>

  <meta name="author" content="Xin Jin">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/xin4.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xin Jin 金鑫</name>
              </p>
              <p>I am an Assistant Professor at the College of Info Science & Technology, <a href="https://www.eitech.edu.cn/" target="_blank">Eastern Institute of Technology (EIT), Ningbo </a> where I cooperated with professor <a href="https://scholar.google.com/citations?user=_cUfvYQAAAAJ&hl=zh-CN" target="_blank">Wenjun Zeng (IEEE Fellow)</a>. Our group is to engage in cutting-edge research in computer vision and multimedia, recently focusing on spatial and embodied AI.
              </p>
              <p>
                 Previously, I was a Visiting Scholar of <a href="http://lv-nus.org/" target="_blank">Learning and Vision (LV) Lab</a> at the National University of Singapore where I was guided by professor <a href="https://sites.google.com/site/sitexinchaowang/" target="_blank">Xinchao Wang</a>, professor <a href="https://sites.google.com/site/jshfeng/home" target="_blank">Jiashi Feng</a> and professor <a href="https://scholar.google.com/citations?hl=zh-CN&user=DNuiPHwAAAAJ" target="_blank">Shuicheng Yan</a>. I received Ph.D. degree from University of Science and Technology of China (USTC), under the supervision of <a href="http://staff.ustc.edu.cn/~chenzhibo/" target="_blank">Zhibo Chen</a>. From Jan. 2019 to Jul. 2020, I also worked at Intelligent Multimedia Group (IMG) in MSRA under the supervision of <a href="https://scholar.google.com/citations?user=XZugqiwAAAAJ&hl=zh-CN" target="_blank">Cuiling Lan</a>. From Sep. 2018 to Jan. 2019, I worked at KDDI Research, Inc. in Japan under the supervision of <a href="https://jp.linkedin.com/in/jianfengxu" target="_blank">Jianfeng Xu</a>.
              </p>
              <p>
                 If you are highly creative, have top research/coding skill and interested in joining us, please do not hesitate to send me (jinxin@eitech.edu.cn) your CV.
              </p>
              <p style="text-align:center">
                <a href="mailto:jinxin@eitech.edu.cn">Email: jinxin@eitech.edu.cn</a> &nbsp;/&nbsp;
<!--                <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp-->
<!--                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp-->
                <a href="https://scholar.google.co.jp/citations?user=byaSC-kAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp;/&nbsp;
				<a href="https://github.com/jinx-USTC">Github</a>
<!--                <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp-->
<!--                <a href="https://github.com/jinx-USTC">Github</a> &nbsp/&nbsp-->
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/JonBarron.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/xin4.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
                <p></p><li> <font color="red"><strong>[08/2025]  Two papers accepted by TPAMI in Aug~</strong></font><p></p>
				<p></p><li> <font color="red"><strong>[06/2025]  Four papers accepted by ICCV 2025~</strong></font><p></p>
              <p></p><li> <font color="red"><strong>[03/2025]  Three papers accepted by CVPR 2025~</strong></font><p></p>
                <p></p><li> <font color="red"><strong>[02/2025]  Two papers accepted by ICLR 2025 (Oral) and IJCAI 2025~</strong></font><p></p>
              <p></p><li> <font color="red"><strong>[10/2024]  Three papers accepted by NeurIPS 2024 (including one spotlight)~</strong></font><p></p>
<!-- 			  <p></p><li> <font color="red"><strong>[07/2024]  Five papers accepted by ECCV 2024~</strong></font><p></p>
			  <p></p><li> <font color="red"><strong>[07/2024]  One paper accepted by IEEE TMM~</strong></font><p></p>
			  <p></p><li> <font color="red"><strong>[05/2024]  One paper accepted by IJCAI 2024~</strong></font><p></p>
			  <p></p><li> <font color="red"><strong>[04/2024]  Two papers accepted by CVPR 2024~</strong></font><p></p>
			  <p></p><li> <font color="red"><strong>[02/2024]  Two papers accepted by AAAI 2024~</strong></font><p></p> -->
<!-- 		  <p></p><li> <font color="red"><strong>[09/2022]  2022 ACM SIGAI China Doctoral Dissertation Award （国际计算机学会中国人工智能分会优博奖）~</strong></font><p></p> 
			  <p></p><li> <font color="red"><strong>[09/2022]  One paper accepted by NeurIPS 2022~</strong></font><p></p>
              <p></p><li> <font color="red"><strong>[07/2022]  Two papers accepted by ECCV 2022~</strong></font><p></p>
              <p></p><li> <font color="red"><strong>[06/2022]  One paper accepted by ACMMM 2022~</strong></font><p></p>
              <p></p><li> <font color="red"><strong>[05/2022]  Chinese Academy of Sciences President's Scholarship-Special Award 中科院院长特别奖~</strong></font><p></p>
              <p></p><li> <font color="red"><strong>[03/2022]  Three papers accepted by CVPR 2022~</strong></font><p></p>
              <p></p></li><li><strong>[12/2021] </strong> Awarded the National Scholarship for Doctoral Students.<p></p>
              <p></p></li><li><strong>[12/2021] </strong> One paper accepted by IEEE TIP 2021.<p></p>
              <p></p></li><li><strong>[08/2021] </strong> One paper accepted by IEEE TMM 2021.<p></p>
              <p></p></li><li><strong>[07/2021] </strong> Two papers accepted by ICCV 2021 (one oral).<p></p>
              <p></p></li><li><strong>[12/2020] </strong> One paper accepted by AAAI 2021.<p></p>
              <p></p></li><li><strong>[07/2020] </strong> Two papers accepted by ECCV 2020.<p></p>
              <p></p></li><li><strong>[03/2020] </strong> Two papers accepted by CVPR 2020.<p></p>
              <p></p></li><li><strong>[11/2019] </strong> Three papers accepted by AAAI 2020.<p></p>
              <p><li><strong>[09/2019] </strong> One paper accepted by NeurIPS 2019.</p>-->
<!--              <p><li><strong>[11/2018] </strong> Awarded the Huawei Scholarship (¥8000).</p>-->
            </li></td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                Recently, in ICCV 2025, we organized the 1st International Workshop and Challenge on Disentangled Representation Learning for Controllable Generation. In CVPR 2024 and ECCV 2024, we organized two tutorial sessions related to “Visual Disentanglement and Compositionality”.
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		

          <tr onmouseout="DreamVLA_stop()" onmouseover="DreamVLA_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id="DreamVLA">
        <img src="images/dreamvla.png" width="160">
      </div>
      <img src="images/dreamvla.png" width="160">
    </div>
    <script type="text/javascript">
      function DreamVLA_start() {
        document.getElementById('DreamVLA').style.opacity = "1";
      }
      function DreamVLA_stop() {
        document.getElementById('DreamVLA').style.opacity = "0";
      }
      DreamVLA_stop();
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2507.04447">
      <papertitle>DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge</papertitle>
    </a>
    <br>
    <a href="">Wenyao Zhang</a>,
    <a href="">Hongsi Liu</a>,
    <a href="">Zekun Qi</a>,
    <a href="">Yunnan Wang</a>,
    <a href="">XinQiang Yu</a>,
    <a href="">Jiazhao Zhang</a>,
    <a href="">Runpei Dong</a>,
    <a href="">Jiawei He</a>,
    <a href="">He Wang</a>,
    <a href="">Zhizheng Zhang</a>,
    <a href="">Li Yi</a>,
    <a href="">Wenjun Zeng</a>,
    <strong>Xin Jin†</strong>
    <br>
    <em><font color="#663399"><strong>ArXiV, 2025</strong></font></em>
    <br>
    <a href="https://arxiv.org/abs/2507.04447">paper</a> /
    <a href="https://github.com/Zhangwenyao1/DreamVLA">code</a> /
    <p></p>
    <p>We recast the vision–language–action model as a perception–prediction–action model and make the model explicitly predict a compact set of dynamic, spatial and high- level semantic information, supplying concise yet comprehensive look-ahead cues for planning.</p>
  </td>
</tr>

<tr onmouseout="BFM_stop()" onmouseover="BFM_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id="BFM">
        <img src="images/bfm.png" width="160">
      </div>
      <img src="images/bfm.png" width="160">
    </div>
    <script type="text/javascript">
      function BFM_start() { document.getElementById('BFM').style.opacity = "1"; }
      function BFM_stop()  { document.getElementById('BFM').style.opacity = "0"; }
      BFM_stop();
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2506.20487">
      <papertitle>Behavior Foundation Model: Towards Next-Generation Whole-Body Control System of Humanoid Robots</papertitle>
    </a>
    <br>
    <a href="">Mingqi Yuan</a>,
    <a href="">Tao Yu</a>,
    <a href="">Wenqi Ge</a>,
    <a href="">Xiuyong Yao</a>,
    <a href="">Dapeng Li</a>,
    <a href="">Huijiang Wang</a>,
    <a href="">Jiayu Chen</a>,
    <strong>Xin Jin†</strong>,
    <a href="">Bo Li</a>,
    <a href="">Hua Chen</a>,
    <a href="">Wei Zhang</a>,
    <a href="">Wenjun Zeng</a>
    <br>
    <em><font color="#663399"><strong>ArXiV, 2025</strong></font></em>
    <br>
    <a href="https://arxiv.org/abs/2506.20487">paper</a>
    <p></p>
    <p>This survey outlines the concept of Behavior Foundation Models (BFMs) for humanoid whole-body control, detailing large-scale pre-training workflows that enable reusable motion primitives and zero-shot adaptation across diverse tasks. It highlights challenges and potential in applying BFMs to real-world systems and curates a growing repository of related works.</p>
  </td>
</tr>

<tr onmouseout="SoFar_stop()" onmouseover="SoFar_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id="SoFar">
        <img src="images/sofar.png" width="160">
      </div>
      <img src="images/sofar.png" width="160">
    </div>
    <script type="text/javascript">
      function SoFar_start() {
        document.getElementById('SoFar').style.opacity = "1";
      }
      function SoFar_stop() {
        document.getElementById('SoFar').style.opacity = "0";
      }
      SoFar_stop();
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2502.13143">
      <papertitle>SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation</papertitle>
    </a>
    <br>
    <a href="">Zekun Qi*</a>,
    <a href="">Wenyao Zhang*</a>,
    <a href="">Yufei Ding*</a>,
    <a href="">Runpei Dong</a>,
    <a href="">Xinqiang Yu</a>,
    <a href="">Jingwen Li</a>,
    <a href="">Lingyun Xu</a>,
    <a href="">Baoyu Li</a>,
    <a href="">Xialin He</a>,
    <a href="">Guofan Fan</a>,
    <a href="">Jiazhao Zhang</a>,
    <a href="">Jiawei He</a>,
    <a href="">Jiayuan Gu</a>,
    <strong>Xin Jin</strong>,
    <a href="">Kaisheng Ma</a>,
    <a href="">Zhizheng Zhang</a>,
    <a href="">He Wang</a>,
    <a href="">Li Yi</a>
    <br>
    <em><font color="#663399"><strong>ArXiV, 2025</strong></font></em>
    <br>
    <a href="https://arxiv.org/abs/2502.13143">paper</a> /
    <a href="https://github.com/qizekun/SoFar">code</a>
    <p></p>
    <p>We introduce the concept of semantic orientation, representing the object orientation condition on open vocabulary language.</p>
  </td>
</tr>

<tr onmouseout="PAI_stop()" onmouseover="PAI_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id="PAI">
        <img src="images/intervla.jpg" width="160">
      </div>
      <img src="images/intervla.jpg" width="160">
    </div>
    <script type="text/javascript">
      function PAI_start() { document.getElementById('PAI').style.opacity = "1"; }
      function PAI_stop()  { document.getElementById('PAI').style.opacity = "0"; }
      PAI_stop();
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://iccv.thecvf.com/Conferences/2025/AcceptedPapers">
      <papertitle>Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions</papertitle>
    </a>
    <br>
    <a href="">Liang Xu</a>,
    <a href="">Chengqun Yang</a>,
    <a href="">Zili Lin</a>,
    <a href="">Fei Xu</a>,
    <a href="">Yifan Liu</a>,
    <a href="">Congsheng Xu</a>,
    <a href="">Yiyi Zhang</a>,
    <a href="">Jie Qin</a>,
    <a href="">Xingdong Sheng</a>,
    <a href="">Yunhui Liu</a>,
    <strong>Xin Jin</strong>,
    <a href="">Yichao Yan</a>,
    <a href="">Wenjun Zeng</a>,
    <a href="">Xiaokang Yang</a>
    <br>
    <em><font color="#663399"><strong>ICCV, 2025</strong></font></em>
    <br>
    <a href="">paper</a>
    <p></p>
    <p>We tackle the problem of how to build and benchmark a large motion model with video action datasets and disentangled rule-based annotations.</p>
  </td>
</tr>

<tr onmouseout="ULTHO_stop()" onmouseover="ULTHO_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id="ULTHO">
        <img src="images/ultho.png" width="160">
      </div>
      <img src="images/ultho.png" width="160">
    </div>
    <script type="text/javascript">
      function ULTHO_start() { document.getElementById('ULTHO').style.opacity = "1"; }
      function ULTHO_stop() { document.getElementById('ULTHO').style.opacity = "0"; }
      ULTHO_stop();
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2503.06101">
      <papertitle>ULTHO: Ultra-Lightweight yet Efficient Hyperparameter Optimization in Deep Reinforcement Learning</papertitle>
    </a>
    <br>
    <a href="">Mingqi Yuan</a>,
    <a href="">Bo Li</a>,
    <strong>Xin Jin†</strong>,
    <a href="">Wenjun Zeng</a>
    <br>
    <em><font color="#663399"><strong>ICCV, 2025</strong></font></em>
    <br>
    <a href="https://arxiv.org/abs/2503.06101">paper</a> 
    <p></p>
    <p>ULTHO formulates hyperparameter optimization for deep RL as a multi-armed bandit over clustered hyperparameter configurations, enabling efficient early pruning of underperforming runs and focusing compute on promising settings. Evaluated on benchmarks like ALE, Procgen, MiniGrid, and PyBullet, ULTHO achieves near‐optimal performance using just a fraction of the usual computational budget.</p>
  </td>
</tr>


<tr onmouseout="DWM_stop()" onmouseover="DWM_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id="DWM">
        <img src="images/DisWM.png" width="160">
      </div>
      <img src="images/DisWM.png" width="160">
    </div>
    <script type="text/javascript">
      function DWM_start() {
        document.getElementById('DWM').style.opacity = "1";
      }
      function DWM_stop() {
        document.getElementById('DWM').style.opacity = "0";
      }
      DWM_stop();
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2503.08751">
      <papertitle>Disentangled World Models: Learning to Transfer Semantic Knowledge from Distracting Videos for Reinforcement Learning</papertitle>
    </a>
    <br>
    <a href="">Qi Wang</a>,
    <a href="">Zhipeng Zhang</a>,
    <a href="">Baao Xie</a>,
    <strong>Xin Jin†</strong>,
    <a href="">Yunbo Wang</a>,
    <a href="">Shiyu Wang</a>,
    <a href="">Liaomo Zheng</a>,
    <a href="">Xiaokang Yang</a>,
    <a href="">Wenjun Zeng</a>
    <br>
    <em><font color="#663399"><strong>ICCV, 2025</strong></font></em>
    <br>
    <a href="https://arxiv.org/abs/2503.08751">paper</a> /
    <a href="https://github.com/qiwang067/diswm">code</a> /
    <p></p>
    <p>Disentangled World Models (DisWM) introduce a model-based RL framework that leverages offline pretraining on distracting videos with disentanglement regularization and offline-to-online latent distillation. This enables transfer of semantic knowledge for improved sample efficiency and generalization in downstream reinforcement learning tasks in visually varied environments.</p>
  </td>
</tr>


<tr onmouseout="HGF_stop()" onmouseover="HGF_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id="HGF">
        <img src="images/hybrid_depth.png" width="160">
      </div>
      <img src="images/hybrid_depth.png" width="160">
    </div>
    <script type="text/javascript">
      function HGF_start() {
        document.getElementById('HGF').style.opacity = "1";
      }
      function HGF_stop() {
        document.getElementById('HGF').style.opacity = "0";
      }
      HGF_stop();
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2505.12345">
      <papertitle>Hybrid‑grained Feature Aggregation with Coarse‑to‑fine Language Guidance for Self‑supervised Monocular Depth Estimation</papertitle>
    </a>
    <br>
    <a href="">Wenyao Zhang*</a>,
    <a href="">Hongsi Liu*</a>,
    <a href="">Bohan Li*</a>,
    <a href="">Jiawei He</a>,
    <a href="">Zekun Qi</a>,
    <a href="">Yunnan Wang</a>,
    <a href="">Shengyang Zhao</a>,
    <a href="">Xinqiang Yu</a>,
    <a href="">Wenjun Zeng</a>,
    <strong>Xin Jin</strong>
    <br>
    <em><font color="#663399"><strong>ICCV, 2025</strong></font></em>
    <br>
    <p></p>
    <p>we propose Hybrid-depth, a novel framework that systematically integrates foundation models (CLIP and DINO) to extract visual priors and acquire sufficient contextual information for self-supervised depth estimation methods.</p>
  </td>
</tr>





<tr onmouseout="BridgeAD_stop()" onmouseover="BridgeAD_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id="BridgeAD">
        <img src="images/bridgead.png" width="160">
      </div>
      <img src="images/bridgead.png" width="160">
    </div>
    <script type="text/javascript">
      function BridgeAD_start() { document.getElementById('BridgeAD').style.opacity = "1"; }
      function BridgeAD_stop()  { document.getElementById('BridgeAD').style.opacity = "0"; }
      BridgeAD_stop();
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2503.14182">
      <papertitle>Bridging Past and Future: End-to-End Autonomous Driving with Historical Prediction and Planning</papertitle>
    </a>
    <br>
    <a href="">Bozhou Zhang</a>,
    <a href="">Nan Song</a>,
    <strong>Xin Jin</strong>,
    <a href="">Li Zhang</a>
    <br>
    <em><font color="#663399"><strong>CVPR, 2025</strong></font></em>
    <br>
    <a href="https://arxiv.org/abs/2503.14182">paper</a> /
    <p></p>
    <p>BridgeAD introduces multi-step motion and planning queries that integrate historical prediction into both perception and planning modules, thereby “bridging” the past and future. With this structure, the model unifies historical insight with current perception and future trajectory planning—achieving state-of-the-art performance on nuScenes in open- and closed-loop settings.</p>
  </td>
</tr>

<tr onmouseout="MLVFF_stop()" onmouseover="MLVFF_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id="MLVFF">
        <img src="images/Ml" width="160">
      </div>
      <img src="images/MLVFF.jpg" width="160">
    </div>
    <script type="text/javascript">
      function MLVFF_start() { document.getElementById('MLVFF').style.opacity = "1"; }
      function MLVFF_stop()  { document.getElementById('MLVFF').style.opacity = "0"; }
      MLVFF_stop();
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2503.06063">
      <papertitle>Multi-Layer Visual Feature Fusion in Multimodal LLMs: Methods, Analysis, and Best Practices</papertitle>
    </a>
    <br>
    <a href="">Junyan Lin</a>,
    <a href="">Haoran Chen</a>,
    <a href="">Yue Fan</a>,
    <a href="">Yingqi Fan</a>,
    <strong>Xin Jin†</strong>,
    <a href="">Hui Su</a>,
    <a href="">Jinlan Fu†</a>,
    <a href="">Xiaoyu Shen</a>
    <br>
    <em><font color="#663399"><strong>CVPR, 2025</strong></font></em>
    <br>
    <a href="https://arxiv.org/abs/2503.06063">paper</a> /
    <a href="https://github.com/EIT-NLP/Layer_Select_Fuse_for_MLLM">code</a>
    <p></p>
    <p>This paper systematically investigates multi-layer visual feature fusion in Multimodal Large Language Models, analyzing optimal layer selection and four fusion strategies. Results show that external direct fusion of features from distinct encoder stages consistently offers the best generalization and stability. Their code is publicly released.</p>
  </td>
</tr>

<tr onmouseout="UniScene_stop()" onmouseover="UniScene_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id="UniScene">
        <img src="images/uniscene.jpg" width="160">
      </div>
      <img src="images/uniscene.jpg" width="160">
    </div>
    <script type="text/javascript">
      function UniScene_start() { document.getElementById('UniScene').style.opacity = "1"; }
      function UniScene_stop()  { document.getElementById('UniScene').style.opacity = "0"; }
      UniScene_stop();
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2412.05435">
      <papertitle>UniScene: Unified Occupancy‑centric Driving Scene Generation</papertitle>
    </a>
    <br>
    <a href="">Bohan Li</a>,
    <a href="">Jiazhe Guo</a>,
    <a href="">Hongsi Liu</a>,
    <a href="">Yingshuang Zou</a>,
    <a href="">Yikang Ding</a>,
    <a href="">Xiwu Chen</a>,
    <a href="">Hu Zhu</a>,
    <a href="">Feiyang Tan</a>,
    <a href="">Chi Zhang</a>,
    <a href="">Tiancai Wang</a>,
    <a href="">Shuchang Zhou</a>,
    <a href="">Li Zhang</a>,
    <a href="">Xiaojuan Qi</a>,
    <a href="">Hao Zhao</a>,
    <a href="">Mu Yang</a>,
    <a href="">Wenjun Zeng</a>,
    <strong>Xin Jin†</strong>
    <br>
    <em><font color="#663399"><strong>CVPR, 2025</strong></font></em>
    <br>
    <a href="https://arxiv.org/abs/2412.05435">paper</a> /
    <a href="https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation">code</a>
    <p></p>
    <p>UniScene proposes the first occupancy‑centric hierarchical framework that consecutively generates semantic occupancy, multi-view video, and LiDAR data from coarse BEV layouts, using Gaussian‑based joint rendering and prior‑guided sparse modeling. This approach significantly outperforms previous methods in fidelity and versatility across all three modalities, benefiting downstream perception tasks :contentReference[oaicite:0]{index=0}.</p>
  </td>
</tr>


<tr onmouseout="LSI_stop()" onmouseover="LSI_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id="LSI">
        <img src="images/LSI.png" width="160">
      </div>
      <img src="images/LSI.png" width="160">
    </div>
    <script type="text/javascript">
      function LSI_start() {
        document.getElementById('LSI').style.opacity = "1";
      }
      function LSI_stop() {
        document.getElementById('LSI').style.opacity = "0";
      }
      LSI_stop();
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2410.03618">
      <papertitle>Open-World Reinforcement Learning over Long Short-Term Imagination</papertitle>
    </a>
    <br>
    <a href="">Jiajian Li*</a>,
    <a href="">Qi Wang*</a>,
    <a href="">Yunbo Wang</a>,
    <strong>Xin Jin</strong>,
    <a href="">Yang Li</a>,
    <a href="">Wenjun Zeng</a>,
    <a href="">Xiaokang Yang</a>
    <br>
    <em><font color="#663399"><strong>ICLR, 2025 Oral</strong></font></em>
    <br>
    <a href="https://arxiv.org/abs/2410.03618">paper</a> /
    <a href="https://github.com/qiwang067/LS-Imagine">code</a>
    <p></p>
    <p>We present LS-Imagine, a model-based RL framework that constructs a long short-term world model combining step-by-step and jumpy transitions, guided by affordance maps to enable goal-conditioned exploration in open-world environments. This approach significantly improves exploration efficiency and sample efficiency in complex, high-dimensional tasks.</p>
  </td>
</tr>

<tr onmouseout="SGDC_stop()" onmouseover="SGDC_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id="SGDC">
        <img src="images/disco.png" width="160">
      </div>
      <img src="images/disco.png" width="160">
    </div>
    <script type="text/javascript">
      function SGDC_start() {
        document.getElementById('SGDC').style.opacity = "1";
      }
      function SGDC_stop() {
        document.getElementById('SGDC').style.opacity = "0";
      }
      SGDC_stop();
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2410.00447">
      <papertitle>Scene Graph Disentanglement and Composition for Generalizable Complex Image Generation</papertitle>
    </a>
    <br>
    <a href="">Yunnan Wang</a>,
    <a href="">Ziqiang Li</a>,
    <a href="">Wenyao Zhang</a>,
    <a href="">Zequn Zhang</a>,
    <a href="">Baao Xie</a>,
    <a href="">Xihui Liu</a>,
    <a href="">Wenjun Zeng</a>,
    <strong>Xin Jin†</strong>
    <br>
    <em><font color="#663399"><strong>NeurIPS, 2024 Spotlight</strong></font></em>
    <br>
    <a href="https://arxiv.org/abs/2410.00447">paper</a> /
    <a href="https://github.com/wangyunnan/DisCo">code</a>
    <p></p>
    <p>This work presents DisCo, a framework that leverages scene graphs as structured conditions to generate complex images by disentangling layouts and semantics via a Semantics-Layout VAE and composing them with a diffusion-based Compositional Masked Attention, while enabling isolated graph-guided editing through a Multi-Layered Sampler—achieving state-of-the-art generalization across diverse scene complexities.</p>
  </td>
</tr>

<tr onmouseout="GUDRL_stop()" onmouseover="GUDRL_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id="GUDRL">
        <img src="images/gem.png" width="160">
      </div>
      <img src="images/gem.png" width="160">
    </div>
    <script type="text/javascript">
      function GUDRL_start() { document.getElementById('GUDRL').style.opacity = "1"; }
      function GUDRL_stop()  { document.getElementById('GUDRL').style.opacity = "0"; }
      GUDRL_stop();
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2407.18999">
      <papertitle>Graph-based Unsupervised Disentangled Representation Learning via Multimodal Large Language Models</papertitle>
    </a>
    <br>
    <a href="">Baao Xie</a>,
    <a href="">Qiuyu Chen</a>,
    <a href="">Yunnan Wang</a>,
    <a href="">Zequn Zhang</a>,
    <strong>Xin Jin†</strong>,
    <a href="">Wenjun Zeng</a>
    <br>
    <em><font color="#663399"><strong>NeurIPS, 2024</strong></font></em>
    <br>
    <a href="https://arxiv.org/abs/2407.18999">paper</a> /
    <p></p>
    <p>This work introduces a bidirectional weighted graph framework that integrates β‑VAE to extract latent factors and leverages multimodal LLMs to detect and weight semantic correlations, leading to fine‐grained, interpretable disentanglement and strong reconstruction performance.</p>
  </td>
</tr>


<tr onmouseout="CoW_stop()" onmouseover="CoW_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id="CoW">
        <img src="images/coworld.png" width="160">
      </div>
      <img src="images/coworld.png" width="160">
    </div>
    <script type="text/javascript">
      function CoW_start() {
        document.getElementById('CoW').style.opacity = "1";
      }
      function CoW_stop() {
        document.getElementById('CoW').style.opacity = "0";
      }
      CoW_stop();
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2305.15260">
      <papertitle>Making Offline RL Online: Collaborative World Models for Offline Visual Reinforcement Learning</papertitle>
    </a>
    <br>
    <a href="">Qi Wang*</a>,
    <a href="">Junming Yang*</a>,
    <a href="">Yunbo Wang</a>,
    <strong>Xin Jin</strong>,
    <a href="">Wenjun Zeng</a>,
    <a href="">Xiaokang Yang</a>
    <br>
    <em><font color="#663399"><strong>NeurIPS, 2024</strong></font></em>
    <br>
    <a href="https://arxiv.org/abs/2305.15260">paper</a> /
    <a href="https://qiwang067.github.io/coworld">code</a>
    <p></p>
    <p>This paper presents CoWorld—a model-based reinforcement learning framework that bridges offline and online domains by leveraging auxiliary simulators as “test beds.” It aligns latent state and reward distributions across domains and introduces min‑max value constraints to mitigate overestimation, yielding substantial improvements over existing offline visual RL methods.</p>
  </td>
</tr>



		<tr onmouseout="GMAA_stop()" onmouseover="GMAA_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="pre">
                <img src="images/GMAA.JPG" width="160">
                </div>
                <img src="images/GMAA.JPG" width="160">
              </div>
              <script type="text/javascript">
                function disen_start() {
                  document.getElementById('cloth').style.opacity = "1";
                }

                function disen_stop() {
                  document.getElementById('cloth').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Discrete Point-wise Attack Is Not Enough: Generalized Manifold Adversarial Attack for Face Recognition</papertitle>
              </a>
              <br>
			  <a href="">Qian Li*</a>,
			  <a href="">Yuxiao Hu*</a>,
			  <a href="">Ye Liu</a>,
			  <a href="">Dongxiao Zhang</a>,
              <strong>Xin Jin†</strong>,
              <a href="">Yuntian Chen†</a>
              <br>
              <em><font color="#663399"><strong>CVPR, 2023</strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2301.06083.pdf">arxiv</a> /
<!--              <a href="data/pre.bib">bibtex</a>-->
              <p></p>
              <p>In this work, by rethinking the inherent relationship between the face of target identity and its variants, we introduce a new pipeline of Generalized Manifold Adversarial Attack (GMAA) to achieve a better attack performance by expanding the attack range.</p>
            </td>
          </tr>		


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
		<tr onmouseout="Task Residual_stop()" onmouseover="Task Residual_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="pre">
                <img src="images/Task Residual.JPG" width="160">
                </div>
                <img src="images/Task Residual.JPG" width="160">
              </div>
              <script type="text/javascript">
                function disen_start() {
                  document.getElementById('cloth').style.opacity = "1";
                }

                function disen_stop() {
                  document.getElementById('cloth').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Task Residual for Tuning Vision-Language Models</papertitle>
              </a>
              <br>
			  <a href="">Tao Yu*</a>,
			  <a href="">Zhihe Lu*</a>,
              <strong>Xin Jin</strong>,
              <a href="">Zhibo Chen</a>,
              <a href="">Xinchao Wang</a>
              <br>
              <em><font color="#663399"><strong>CVPR, 2023</strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2211.10277.pdf">arxiv</a> /
              <a href="https://github.com/geekyutao/TaskRes">code</a> /
<!--              <a href="data/pre.bib">bibtex</a>-->
              <p></p>
              <p>In this work, we propose a new efficient tuning approach for VLMs named Task Residual Tuning (TaskRes), which performs directly on the text-based classifier and explicitly decouples the prior knowledge of the pre-trained models and new knowledge regarding a target task.</p>
            </td>
          </tr>				  
		  
		  
		    <tr onmouseout="Causality_stop()" onmouseover="Causality_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="pre">
                <img src="images/Causality.JPG" width="160">
                </div>
                <img src="images/Causality.JPG" width="160">
              </div>
              <script type="text/javascript">
                function disen_start() {
                  document.getElementById('cloth').style.opacity = "1";
                }

                function disen_stop() {
                  document.getElementById('cloth').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Learning Distortion Invariant Representation for Image Restoration from A Causality Perspective</papertitle>
              </a>
              <br>
			  <a href="">Xin Li*</a>,
			  <a href="">Bingchen Li*</a>,
              <strong>Xin Jin</strong>,
              <a href="">Cuiling Lan</a>,
              <a href="">Zhibo Chen</a>
              <br>
              <em><font color="#663399"><strong>CVPR, 2023</strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2303.06859.pdf">arxiv</a> /
              <a href="https://github.com/lixinustc/causal-IR-DIL">code</a> /
<!--              <a href="data/pre.bib">bibtex</a>-->
              <p></p>
              <p>In this work, we are the first to propose a novel training strategy for image restoration from the causality perspective, to improve the generalization ability of DNNs for unknown degradations.</p>
            </td>
          </tr>				  
		  
            <tr onmouseout="DDB_stop()" onmouseover="DDB_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="pre">
                <img src="images/DDB.JPG" width="160">
                </div>
                <img src="images/DDB.JPG" width="160">
              </div>
              <script type="text/javascript">
                function disen_start() {
                  document.getElementById('cloth').style.opacity = "1";
                }

                function disen_stop() {
                  document.getElementById('cloth').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Deliberated Domain Bridging for Domain Adaptive Semantic Segmentation</papertitle>
              </a>
              <br>
			  <a href="">Lin Chen*</a>,
			  <a href="">Zhixiang Wei*</a>,
              <strong>Xin Jin*(equal)</strong>,
              <a href="">Huaian Chen</a>,
              <a href="">Kai Chen</a>,
              <a href="">Yi Jin </a>
              <br>
              <em><font color="#663399"><strong>NeurIPS, 2022</strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2209.07695.pdf">arxiv</a> /
              <a href="https://github.com/xiaoachen98/DDB">code</a> /
<!--              <a href="data/pre.bib">bibtex</a>-->
              <p></p>
              <p>In this work, we resort to data mixing to establish a deliberated domain bridging (DDB) for domain adaptive semantic segmentation. The joint distributions of source and target domains are aligned and interacted with each other in the intermediate space.</p>
            </td>
          </tr>				  

            <tr onmouseout="omni_stop()" onmouseover="omni_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="pre">
                <img src="images/omni.JPG" width="160">
                </div>
                <img src="images/omni.JPG" width="160">
              </div>
              <script type="text/javascript">
                function disen_start() {
                  document.getElementById('cloth').style.opacity = "1";
                }

                function disen_stop() {
                  document.getElementById('cloth').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Image Coding for Machines with Omnipotent Feature Learning</papertitle>
              </a>
              <br>
			  <a href="">Ruoyu Feng*</a>,
              <strong>Xin Jin*(equal)</strong>,
              <a href="">Zongyu Guo</a>,
              <a href="">Runsen Feng</a>,
              <a href="">Yixin Gao </a>,
              <a href="">Tianyu He </a>,
              <a href="">Zhizheng Zhang </a>,
              <a href="">Simeng Sun </a>,
              <a href="http://staff.ustc.edu.cn/~chenzhibo/">Zhibo Chen</a>
              <br>
              <em><font color="#663399"><strong>ECCV, 2022</strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/abs/2207.01932">arxiv</a> /
<!--              <a href="https://github.com/Frost-Yang-99/UP-ReID">code</a> /
<!--              <a href="data/pre.bib">bibtex</a>-->
              <p></p>
              <p>In this paper, we attempt to learn a kind of omnipotent feature that is both general (for AI tasks) and compact (for compression) for Image Coding for Machines (ICM). Considering self-supervised learning (SSL) improves feature generalization, we integrate it with the compression task to learn such features.</p>
            </td>
          </tr>		
		  

            <tr onmouseout="jingwen_stop()" onmouseover="jingwen_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="pre">
                <img src="images/jingwen.JPG" width="160">
                </div>
                <img src="images/jingwen.JPG" width="160">
              </div>
              <script type="text/javascript">
                function disen_start() {
                  document.getElementById('cloth').style.opacity = "1";
                }

                function disen_stop() {
                  document.getElementById('cloth').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Learning with Recoverable Forgetting</papertitle>
              </a>
              <br>
			  <a href="https://scholar.google.com/citations?user=8GQnNP0AAAAJ&hl=zh-CN">Jingwen Ye</a>,
			  <a href="">Yifang Fu</a>,
			  <a href="">Jie Song</a>,
			  <a href="">Xingyi Yang</a>,
			  <a href="">Songhua Liu</a>,
              <strong>Xin Jin</strong>,
              <a href="">Mingli Song</a>,
              <a href="https://sites.google.com/site/sitexinchaowang/">Xinchao Wang</a>
              <br>
              <em><font color="#663399"><strong>ECCV, 2022</strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/abs/2207.08224">arxiv</a> /
<!--              <a href="https://github.com/Frost-Yang-99/UP-ReID">code</a> /
<!--              <a href="data/pre.bib">bibtex</a>-->
              <p></p>
              <p>In this paper, we explore a novel learning scheme, termed as Learning wIth Recoverable Forgetting (LIRF), that explicitly handles the task- or sample-specific knowledge removal and recovery.</p>
            </td>
          </tr>			  
		
            <tr onmouseout="mcl_stop()" onmouseover="mcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="pre">
                <img src="images/mcl.JPG" width="160">
                </div>
                <img src="images/mcl.JPG" width="160">
              </div>
              <script type="text/javascript">
                function disen_start() {
                  document.getElementById('cloth').style.opacity = "1";
                }

                function disen_stop() {
                  document.getElementById('cloth').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Meta Clustering Learning for Large-scale Unsupervised Person Re-identification</papertitle>
              </a>
              <br>
              <strong>Xin Jin</strong>,
              <a href="http://home.ustc.edu.cn/~hetianyu/">Tianyu He</a>,
              <a href="">Xu Shen</a>,
              <a href="https://tongliang-liu.github.io/">Tongliang Liu</a>,
              <a href="https://sites.google.com/site/sitexinchaowang/">Xinchao Wang </a>,
              <a href="">Jianqiang Huang </a>,
              <a href="http://staff.ustc.edu.cn/~chenzhibo/">Zhibo Chen</a>,
              <a href="https://scholar.google.com/citations?user=6G-l4o0AAAAJ&hl=zh-CN">Xian-Sheng Hua </a>

              <br>
              <em><font color="#663399"><strong>ACMMM, 2022</strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2111.10032.pdf">arxiv</a> /
<!--              <a href="https://github.com/Frost-Yang-99/UP-ReID">code</a> /
<!--              <a href="data/pre.bib">bibtex</a>-->
              <p></p>
              <p>In this paper, we make attempt to the large-scale Unsupervised ReID and propose a “small data for big task” paradigm dubbed Meta Clustering Learning (MCL), which our method significantly saves computational cost while achieving a comparable or even better performance compared to prior works.</p>
            </td>
          </tr>		
		  
            <tr onmouseout="pre_stop()" onmouseover="pre_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="pre">
                <img src="images/pre.png" width="160">
                </div>
                <img src="images/pre.png" width="160">
              </div>
              <script type="text/javascript">
                function disen_start() {
                  document.getElementById('cloth').style.opacity = "1";
                }

                function disen_stop() {
                  document.getElementById('cloth').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Unleashing the Potential of Unsupervised Pre-Training with Intra-Identity Regularization for Person Re-Identification</papertitle>
              </a>
              <br>
              <a href="">Zizheng Yang</a>,
              <strong>Xin Jin</strong>,
              <a href="http://home.ustc.edu.cn/~zkcys001/">Kecheng Zheng</a>,
              <a href="">Feng Zhao</a>

              <br>
              <em><font color="#663399"><strong>CVPR, 2022</strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/abs/2112.00317">arxiv</a> /
              <a href="https://github.com/Frost-Yang-99/UP-ReID">code</a> /
<!--              <a href="data/pre.bib">bibtex</a>-->
              <p></p>
              <p>We design an Unsupervised Pre-training framework for ReID based on the contrastive learning (CL) pipeline, dubbed UP-ReID.</p>
            </td>
          </tr>


          <tr onmouseout="cloth_stop()" onmouseover="cloth_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="cloth">
                <img src="images/cloth.jpg" width="160">
                </div>
                <img src="images/cloth.jpg" width="160">
              </div>
              <script type="text/javascript">
                function disen_start() {
                  document.getElementById('cloth').style.opacity = "1";
                }

                function disen_stop() {
                  document.getElementById('cloth').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Cloth-Changing Person Re-identification from A Single Image with Gait Prediction and Regularization</papertitle>
              </a>
              <br>
              <strong>Xin Jin</strong>,
              <a href="http://home.ustc.edu.cn/~hetianyu/">Tianyu He</a>,
              <a href="http://home.ustc.edu.cn/~zkcys001/">Kecheng Zheng</a>,
              <a href="">Zhiheng Ying</a>,
              <a href="">Xu Shen</a>,
              <a href="">Zhen Huang </a>,
              <a href="">Ruoyu Feng </a>,
              <a href="">Jianqiang Huang </a>,
              <a href="">Xian-Sheng Hua </a>,
              <a href="http://staff.ustc.edu.cn/~chenzhibo/">Zhibo Chen</a>

              <br>
              <em><font color="#663399"><strong>CVPR, 2022</strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2103.15537.pdf">arxiv</a> /
              <a href="https://github.com/jinx-USTC/GI-ReID">code</a> /
<!--              <a href="data/cloth.bib">bibtex</a>-->
              <p></p>
              <p>We focus on handling well the Cloth-Changing ReID problem under a more challenging setting, i.e., just from a single image, which enables high-efficiency and latency-free pedestrian identify for real-time surveillance applications.</p>
            </td>
          </tr>

          <tr onmouseout="ReUSE_stop()" onmouseover="ReUSE_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="ReUSE">
                <img src="images/ReUSE.jpg" width="160">
                </div>
                <img src="images/ReUSE.jpg" width="160">
              </div>
              <script type="text/javascript">
                function disen_start() {
                  document.getElementById('ReUSE').style.opacity = "1";
                }

                function disen_stop() {
                  document.getElementById('ReUSE').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Reusing the Task-specific Classifier as a Discriminator: Discriminator-free Adversarial Domain Adaptation</papertitle>
              </a>
              <br>
              <a href="">Lin Chen</a>,
              <a href="">Huaian Chen</a>,
              <a href="">Zhixiang Wei</a>,
              <strong>Xin Jin</strong>,
              <a href="">Xiao Tan</a>,
              <a href="">Yi Jin</a>,
              <a href="">Enhong Chen </a>
              <br>
              <em><font color="#663399"><strong>CVPR, 2022</strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2204.03838.pdf">arxiv</a> /
              <a href="https://github.com/xiaoachen98/DALN">code</a> /
<!--              <a href="data/cloth.bib">bibtex</a>-->
              <p></p>
              <p>We address the adversarial-based DA problem from a different perspective and design a simple yet effective adversarial paradigm in the form of a discriminator-free adversarial learning network (DALN), wherein the category classifier is reused as a discriminator.</p>
            </td>
          </tr>

          <tr onmouseout="DP_stop()" onmouseover="DP_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="DP">
                <img src="images/DP.jpg" width="160">
                </div>
                <img src="images/DP.jpg" width="160">
              </div>
              <script type="text/javascript">
                function unc_start() {
                  document.getElementById('DP').style.opacity = "1";
                }

                function unc_stop() {
                  document.getElementById('DP').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Dual Prior Learning for Blind and Blended Image Restoration</papertitle>
              </a>
              <br>
              <strong>Xin Jin</strong>,
              <a href="https://www.robots.ox.ac.uk/~lz/">Li Zhang</a>,
              <a href="">Chaowei Shan</a>,
              <a href="">Xin Li</a>,
              <a href="http://staff.ustc.edu.cn/~chenzhibo/">Zhibo Chen</a>


              <br>
              <em><font color="#663399"><strong>IEEE TIP, 2021</strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9663408/figures#figures">paper</a> /
<!--              <a href="https://github.com/zkcys001/CFD">code</a> /-->
<!--              <a href="data/CFD.bib">bibtex</a>-->
              <p></p>
              <p>We propose the Dual Prior Learning (DPL) method for blind image restoration by taking both image and distortion priors into account. DPL goes beyond DIP (deep image prior) by considering an additional step to explicitly learn the blended distortion prior.</p>
            </td>
          </tr>

          <tr onmouseout="SNRTMM_stop()" onmouseover="SNRTMM_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="SNRTMM">
                <img src="images/SNRTMM.jpg" width="160">
                </div>
                <img src="images/SNRTMM.jpg" width="160">
              </div>
              <script type="text/javascript">
                function unc_start() {
                  document.getElementById('SNRTMM').style.opacity = "1";
                }

                function unc_stop() {
                  document.getElementById('SNRTMM').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Style Normalization and Restitution for DomainGeneralization and Adaptation</papertitle>
              </a>
              <br>
              <strong>Xin Jin</strong>,
              <a href="">Cuiling Lan</a>,
              <a href="">Wenjun Zeng</a>,
              <a href="">Zhibo Chen</a>

              <br>
              <em><font color="#663399"><strong>IEEE TMM, 2021</strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2101.00588">paper</a> /
              <a href="https://github.com/microsoft/SNR">code</a> /
<!--              <a href="data/.bib">bibtex</a>-->
              <p></p>
              <p>We design a novel Style Normalization and Restitution module (SNR) to simultaneously ensure both high generalization and discrimination capability of the networks, and evaluate it on multiple vision tasks of classification, detection, segmentation, etc.</p>
            </td>
          </tr>

          <tr onmouseout="RADA_stop()" onmouseover="RADA_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="RADA">
                <img src="images/RADA.jpg" width="160">
                </div>
                <img src="images/RADA.jpg" width="160">
              </div>
              <script type="text/javascript">
                function unc_start() {
                  document.getElementById('RADA').style.opacity = "1";
                }

                function unc_stop() {
                  document.getElementById('RADA').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Re-energizing Domain Discriminator with Sample Relabeling for Adversarial Domain Adaptation</papertitle>
              </a>
              <br>
              <strong>Xin Jin</strong>,
              <a href="">Cuiling Lan</a>,
              <a href="">Wenjun Zeng</a>,
              <a href="">Zhibo Chen</a>

              <br>
              <em><font color="#663399"><strong>ICCV, 2021</strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="http://openaccess.thecvf.com/content/ICCV2021/papers/Jin_Re-Energizing_Domain_Discriminator_With_Sample_Relabeling_for_Adversarial_Domain_Adaptation_ICCV_2021_paper.pdf">paper</a> /
<!--              <a href="">code(coming soon)</a> /-->
<!--              <a href="data/.bib">bibtex</a>-->
              <p></p>
              <p>We propose an efficient optimization strategy named Re-enforceable Adversarial Domain Adaptation (RADA) which aims to re-energize the domain discriminator during the training by using dynamic domain labels.</p>
            </td>
          </tr>

          <tr onmouseout="dense_stop()" onmouseover="dense_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="pos">
                <img src="images/dense.jpg" width="160">
                </div>
                <img src="images/dense.jpg" width="160">
              </div>
              <script type="text/javascript">
                function unc_start() {
                  document.getElementById('dense').style.opacity = "1";
                }

                function unc_stop() {
                  document.getElementById('dense').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Dense Interaction Learning for Video-based Person Re-identification</papertitle>
              </a>
              <br>
              <a href="">Tianyu He</a>,
              <strong>Xin Jin</strong>,
              <a href="">Xu Shen</a>,
              <a href="">Jianqiang Huang</a>,
              <a href="">Zhibo Chen</a>,
              <a href="">Xian-Sheng Hua</a>

              <br>
              <em><font color="#663399"><strong>ICCV, 2021 (Oral)</strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/He_Dense_Interaction_Learning_for_Video-Based_Person_Re-Identification_ICCV_2021_paper.pdf">paper</a> /
<!--              <a href="https://github.com/zkcys001/UDAStrongBaseline">code</a> /-->
<!--              <a href="data/pos2021.bib">bibtex</a>-->
              <p></p>
              <p>This paper proposes a hybrid framework, Dense Interaction Learning (DenseIL), that takes the principal advantages of both CNN-based and Attention-based architectures to tackle video-based person re-ID difficulties.</p>
            </td>
          </tr>

          <tr onmouseout="Omni_stop()" onmouseover="Omni_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="OmniSR">
                <img src="images/OmniSR.jpg" width="160">
                </div>
                <img src="images/OmniSR.jpg" width="160">
              </div>
              <script type="text/javascript">
                function Omni_start() {
                  document.getElementById('OmniSR').style.opacity = "1";
                }

                function Omni_stop() {
                  document.getElementById('OmniSR').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Learning Omni-frequency Region-adaptive Representations for Real Image Super-Resolution</papertitle>
              </a>
              <br>
              <a href="">Xin Li*</a>,
              <strong>Xin Jin*</strong>,
              <a href="">Tao Yu</a>,
              <a href="">Yingxue Pang</a>,
              <a href="">Simeng Sun</a>,
              <a href="">Zhizheng Zhang</a>,
              <a href="">Zhibo Chen</a>

              <br>*Equal Contribution
              <em><font color="#663399"><strong>AAAI, 2021</strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://www.aaai.org/AAAI21Papers/AAAI-3085.LiX.pdf">arxiv</a> /
<!--              <a href="https://github.com/zkcys001/UDAStrongBaseline">code</a> /-->
<!--              <a href="data/GLT2021.bib">bibtex</a>-->
              <p></p>
              <p>The key to solving this more challenging real image super-resolution (RealSR) problem lies in learning feature representations that are both informative and content-aware. We propose an Omni-frequency Region-adaptive Network (OR-Net), here we call features of all low, middle and high frequencies omni-frequency features.</p>
            </td>
          </tr>

          <tr onmouseout="ECCV1_stop()" onmouseover="ECCV1_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="ECCV1">
                <img src="images/ECCV1.jpg" width="160">
                </div>
                <img src="images/ECCV1.jpg" width="160">
              </div>
              <script type="text/javascript">
                function ECCV1_start() {
                  document.getElementById('ECCV1').style.opacity = "1";
                }

                function ECCV1_stop() {
                  document.getElementById('ECCV1').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Global Distance-distributions Separation for Unsupervised Person Re-identification</papertitle>
              </a>
              <br>
              <strong>Xin Jin</strong>,
              <a href="">Jiawei Liu</a>,
              <a href="">Cuiling Lan</a>,
              <a href="">Wenjun Zeng</a>,
              <a href="">Zhibo Chen</a>

              <br>
              <em><font color="#663399"><strong>ECCV, 2020</strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2006.00752">paper</a> /
<!--              <a href="">code(coming soon)</a> /-->
<!--              <a href="data/SCTL2021.bib">bibtex</a>-->
              <p></p>
              <p>We introduce a global distance-distributions separation (GDS) constraint over the two distributions to encourage the clear separation of positive and negative samples from a global view.</p>
            </td>
          </tr>

          <tr onmouseout="ECCV2_stop()" onmouseover="ECCV2_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="ECCV2">
                <img src="images/ECCV2.jpg" width="160">
                </div>
                <img src="images/ECCV2.jpg" width="160">
              </div>
              <script type="text/javascript">
                function ECCV2_start() {
                  document.getElementById('ECCV2').style.opacity = "1";
                }

                function ECCV2_stop() {
                  document.getElementById('ECCV2').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Learning Disentangled Feature Representation for Hybrid-distorted Image Restoration</papertitle>
              </a>
              <br>
              <a href="">Xin Li </a>,
              <strong>Xin Jin</strong>,
              <a href="">Jianxin Lin</a>,
              <a href="">Tao Yu</a>,
              <a href="">Sen Liu </a>,
              <a href="">Yaojun Wu </a>,
              <a href="">Wei Zhou</a>,
              <a href="">Zhibo Chen </a>

              <br>
              <em><font color="#663399"><strong>ECCV, 2020</strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2007.11430">paper</a> /
<!--              <a href="">code(coming soon)</a> /-->
<!--              <a href="data/disen.bib">bibtex</a>-->
              <p></p>
              <p>We introduce the concept of Disentangled Feature Learning to achieve the feature-level divide-and-conquer of hybrid distortions for low-level enhancement.</p>
            </td>
          </tr>

          <tr onmouseout="SNR_stop()" onmouseover="SNR_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="SNR">
                <img src="images/SNR.jpg" width="160">
                </div>
                <img src="images/SNR.jpg" width="160">
              </div>
              <script type="text/javascript">
                function SNR_start() {
                  document.getElementById('SNR').style.opacity = "1";
                }
                function SNR_stop() {
                  document.getElementById('SNR').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Style normalization and restitution for generalizable person re-identification</papertitle>
              </a>
              <br>
              <strong>Xin Jin</strong>,
              <a href="">Cuiling Lan</a>,
              <a href="">Wenjun Zeng</a>,
              <a href="">Zhibo Chen</a>,
              <a href="">Li Zhang</a>

              <br>
              <em><font color="#663399"><strong>CVPR, 2020</strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Jin_Style_Normalization_and_Restitution_for_Generalizable_Person_Re-Identification_CVPR_2020_paper.pdf">paper</a> /
              <a href="https://github.com/microsoft/SNR">code</a> /
<!--              <a href="data/unc2021.bib">bibtex</a>-->
              <p></p>
              <p>We propose a simple yet effective Style Normalization and Restitution (SNR) module. Specifically, we filter out style variations (eg, illumination, color contrast) by Instance Normalization (IN). However, such a process inevitably removes discriminative information. We propose to distill identity-relevant feature from the removed information and restitute it to the network to ensure high discrimination.</p>
            </td>
          </tr>

          <tr onmouseout="att_stop()" onmouseover="att_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="att">
                <img src="images/att.jpg" width="160">
                </div>
                <img src="images/att.jpg" width="160">
              </div>
              <script type="text/javascript">
                function att_start() {
                  document.getElementById('att').style.opacity = "1";
                }

                function att_stop() {
                  document.getElementById('att').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Relation-Aware Global Attention</papertitle>
              </a>
              <br>
              <a href="">Zhizheng Zhang</a>,
              <a href="">Cuiling Lan</a>,
              <a href="">Wenjun Zeng</a>,
              <strong>Xin Jin</strong>,
              <a href="">Zhibo Chen</a>
              <br>

              <br>
              <em>CVPR</em>, 2020 &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://scholar.google.co.jp/scholar?oi=bibs&cluster=15864607000188383728&btnI=1&hl=zh-CN">paper</a> /
              <a href="https://github.com/microsoft/Relation-Aware-Global-Attention-Networks">code</a> /
<!--              <a href="">bibtex</a>-->
              <p></p>
              <p>We propose an effective Relation-Aware Global Attention (RGA) module which captures the global structural information for better attention learning.</p>
            </td>
          </tr>

          <tr onmouseout="SAN_stop()" onmouseover="SAN_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="SAN">
                <img src="images/SAN.jpg" width="160">
                </div>
                <img src="images/SAN.jpg" width="160">
              </div>
              <script type="text/javascript">
                function SAN_start() {
                  document.getElementById('SAN').style.opacity = "1";
                }

                function SAN_stop() {
                  document.getElementById('SAN').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Semantics-aligned representation learning for person re-identification</papertitle>
              </a>
              <br>

              <strong>Xin Jin</strong>,
              <a href="">Cuiling Lan</a>,
              <a href="">Wenjun Zeng</a>,
              <a href="">Guoqiang Wei</a>,
              <a href="">Zhibo Chen</a>

              <br>
              <em><font color="#663399"><strong>AAAI, 2020 </strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://ojs.aaai.org/index.php/AAAI/article/download/6775/6629">paper</a> /
              <a href="https://github.com/microsoft/Semantics-Aligned-Representation-Learning-for-Person-Re-identification">code</a> /
<!--              <a href="data/htp2019.bib">bibtex</a>-->
              <p></p>
              <p>We build a Semantics Aligning Network (SAN) which consists of a base network as encoder (SA-Enc) for re-ID, and a decoder (SA-Dec) for reconstructing the densely semantics aligned full texture image.</p>
            </td>
          </tr>

          <tr onmouseout="UMTS_stop()" onmouseover="UMTS_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="UMTS">
                <img src="images/UMTS.jpg" width="160">
                </div>
                <img src="images/UMTS.jpg" width="160">
              </div>
              <script type="text/javascript">
                function UMTS_start() {
                  document.getElementById('UMTS').style.opacity = "1";
                }

                function UMTS_stop() {
                  document.getElementById('UMTS').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Uncertainty-Aware Multi-Shot Knowledge Distillation for Image-Based Object Re-Identification</papertitle>
              </a>
              <br>
              <strong>Xin Jin</strong>,
              <a href="">Cuiling Lan</a>,
              <a href="">Wenjun Zeng</a>,
              <a href="">Zhibo Chen</a>

              <br>
               <em><font color="#663399"><strong>AAAI, 2020 </strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://ojs.aaai.org/index.php/AAAI/article/download/6774/6628">paper</a> /
<!--              <a href="https://github.com/zkcys001/distracting_feature">code</a> /-->
<!--              <a href="data/abs2019.bib">bibtex</a>-->
              <p></p>
              <p>We propose exploiting the multi-shots of the same identity to guide the feature learning of each individual image. Specifically, we design an Uncertainty-aware Multi-shot Teacher-Student (UMTS) Network.</p>
            </td>
          </tr>

          <tr onmouseout="RN_stop()" onmouseover="RN_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id="RN">
                <img src="images/RN.jpg" width="160">
                </div>
                <img src="images/RN.jpg" width="160">
              </div>
              <script type="text/javascript">
                function RN_start() {
                  document.getElementById('RN').style.opacity = "1";
                }

                function RN_stop() {
                  document.getElementById('RN').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Region Normalization for Image Inpainting</papertitle>
              </a>
              <br>
              <a href="">Tao Yu</a>,
              <a href="">Zongyu Guo</a>,
              <strong>Xin Jin</strong>,
              <a href="">Shilin Wu</a>,
              <a href="">Zhibo Chen</a>,
              <a href="">Weiping Li</a>,
              <a href="">Zhizheng Zhang</a>,
              <a href="">Sen Liu</a>
              <br>

              <br>
              <em><font color="#663399"><strong>AAAI, 2020 </strong></font></em> &nbsp; <font color="red"><strong></strong></font>
              <br>
              <a href="https://ojs.aaai.org/index.php/AAAI/article/download/6967/6821">paper</a> /
              <a href="https://github.com/geekyutao/RN">code</a>
              <p></p>
              <p>We show that the mean and variance shifts caused by full-spatial FN limit the image inpainting network training and we propose a spatial region-wise normalization named Region Normalization (RN) to overcome the limitation.</p>
            </td>
          </tr>

<!--          <tr onmouseout="layout_stop()" onmouseover="layout_start()">-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id="layout">-->
<!--                <img src="images/layout.jpg" width="160">-->
<!--                </div>-->
<!--                <img src="images/layout.jpg" width="160">-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function layout_stop() {-->
<!--                  document.getElementById('layout').style.opacity = "1";-->
<!--                }-->

<!--                function layout_start() {-->
<!--                  document.getElementById('layout').style.opacity = "0";-->
<!--                }-->
<!--                unprocessing_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="">-->
<!--                <papertitle>LA-Net: Layout-Aware Dense Network for Monocular Depth Estimation</papertitle>-->
<!--              </a>-->
<!--              <br>-->

<!--              <strong>Kecheng Zheng</strong>,-->
<!--              <a href="https://dblp.uni-trier.de/search?q=zheng-jun+zha/">Zheng-jun Zha</a>,-->
<!--              <a href="">Yang Cao</a>,-->
<!--              <a href="">Xuejin Chen</a>,-->
<!--              <a href="">Feng Wu</a>,-->

<!--              <br>-->
<!--              <em><font color="#663399"><strong>ACM MM, 2018</strong></font></em> &nbsp; <font color="red"><strong>Oral Presentation!</strong></font>-->
<!--              <br>-->
<!--              <a href="https://dl.acm.org/doi/abs/10.1145/3240508.3240628">acm</a> /-->
<!--              <a href="data/layout2019.bib">bibtex</a>-->
<!--              <p></p>-->
<!--              <p>We propose a novel Layout-Aware Convolutional Neural Network (LA-Net) for accurate monocular depth estimation by simultaneously perceiving scene layout and local depth details.</p>-->
<!--            </td>-->
          </tr></tbody></table>


<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--            <tr>-->
<!--            <td style="padding:20px;width:100%;vertical-align:middle">-->
<!--              <heading>CodeBase</heading>-->
<!--              <p>-->
<!--                Project participant: <a href="https://github.com/JDAI-CV/fast-reid">FastReID: a Pytorch Toolbox for General Instance Re-identification</a>, <font color="red"><strong>Github 1.9k+ star</strong></font>-->
<!--              </p>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
<!--            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>-->
            <td width="100%" valign="center">
              Invited Reviewer for IEEE TIP, IEEE TNNLS, IEEE TIP, IEEE TCSVT, Pattern Recognition
              <br><br>
              Invited Reviewer for NeurIPS-2022, ECCV-2022, ACMMM-2022, CVPR-2022, AAAI-2022 (PC), ICCV-2021, CVPR-2021, AAAI-2021, ACMMM-2020, VCIP-2020, etc.
<!--              <br><br>-->
<!--              Invited Reviewer for International Conference on Acoustics, Speech and Signal Processing (ICASSP 2019, ICASSP 2020)-->
<!--              <br><br>-->
<!--              Invited Reviewer for ACM International Conference on Multimedia (ACM MM 2020, ACM MM 2021)-->
<!--              <br><br>-->
<!--              Invited Reviewer for International Conference on Pattern Recognition (ICPR 2020)-->
<!--              <br><br>-->
<!--              Invited Reviewer for AAAI Conference on Artificial Intelligence (AAAI 2021, AAAI 2022)-->
<!--              <br><br>-->
<!--              Invited Reviewer for IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2022)-->

<!--              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair, CVPR 2021</a>-->
<!--              <br><br>-->
<!--              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>-->
<!--              <br><br>-->
<!--              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>-->
            </td>
          </tr>

        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.

                <br>
              </p>
            </td>
          </tr>
        </tbody></table>


      </td>
    </tr>
  </tbody></table>



</body></html>
